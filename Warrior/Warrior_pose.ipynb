{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "251d784c-117f-4405-aaba-0f6389568e78",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "import numpy as np\n",
    "from collections import OrderedDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0ab8cc67-d482-44fe-8f3b-29c2e2f1c78e",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Initialize MediaPipe Pose\n",
    "mp_pose = mp.solutions.pose\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "pose = mp_pose.Pose()\n",
    "\n",
    "# Define adjustable thresholds (can be edited easily)\n",
    "THRESHOLDS = {\n",
    "    \"front_knee_angle\": (70, 120),\n",
    "    \"back_leg_angle\": (150, 180),\n",
    "    \"hip_orientation\": (0, 30),\n",
    "    \"torso_angle\": (0, 100),\n",
    "    \"arm_angle\": (160, 180),\n",
    "    \"shoulder_hip_alignment\": (0, 40)\n",
    "}\n",
    "\n",
    "# Function to calculate angle between three points\n",
    "def calculate_angle(p1, p2, p3):\n",
    "    a = np.array(p1)  # First point\n",
    "    b = np.array(p2)  # Vertex\n",
    "    c = np.array(p3)  # Third point\n",
    "    ba = a - b\n",
    "    bc = c - b\n",
    "    cosine_angle = np.dot(ba, bc) / (np.linalg.norm(ba) * np.linalg.norm(bc))\n",
    "    angle = np.arccos(cosine_angle) * 180 / np.pi\n",
    "    return angle\n",
    "\n",
    "# Function to check pose and return errors\n",
    "def check_warrior_pose(landmarks):\n",
    "    errors = []\n",
    "    \n",
    "    # Extract key landmarks (x, y coordinates)\n",
    "    l_hip = [landmarks[mp_pose.PoseLandmark.LEFT_HIP].x, landmarks[mp_pose.PoseLandmark.LEFT_HIP].y]\n",
    "    r_hip = [landmarks[mp_pose.PoseLandmark.RIGHT_HIP].x, landmarks[mp_pose.PoseLandmark.RIGHT_HIP].y]\n",
    "    l_knee = [landmarks[mp_pose.PoseLandmark.LEFT_KNEE].x, landmarks[mp_pose.PoseLandmark.LEFT_KNEE].y]\n",
    "    r_knee = [landmarks[mp_pose.PoseLandmark.RIGHT_KNEE].x, landmarks[mp_pose.PoseLandmark.RIGHT_KNEE].y]\n",
    "    l_ankle = [landmarks[mp_pose.PoseLandmark.LEFT_ANKLE].x, landmarks[mp_pose.PoseLandmark.LEFT_ANKLE].y]\n",
    "    r_ankle = [landmarks[mp_pose.PoseLandmark.RIGHT_ANKLE].x, landmarks[mp_pose.PoseLandmark.RIGHT_ANKLE].y]\n",
    "    l_shoulder = [landmarks[mp_pose.PoseLandmark.LEFT_SHOULDER].x, landmarks[mp_pose.PoseLandmark.LEFT_SHOULDER].y]\n",
    "    r_shoulder = [landmarks[mp_pose.PoseLandmark.RIGHT_SHOULDER].x, landmarks[mp_pose.PoseLandmark.RIGHT_SHOULDER].y]\n",
    "    l_elbow = [landmarks[mp_pose.PoseLandmark.LEFT_ELBOW].x, landmarks[mp_pose.PoseLandmark.LEFT_ELBOW].y]\n",
    "    r_elbow = [landmarks[mp_pose.PoseLandmark.RIGHT_ELBOW].x, landmarks[mp_pose.PoseLandmark.RIGHT_ELBOW].y]\n",
    "    l_wrist = [landmarks[mp_pose.PoseLandmark.LEFT_WRIST].x, landmarks[mp_pose.PoseLandmark.LEFT_WRIST].y]\n",
    "    r_wrist = [landmarks[mp_pose.PoseLandmark.RIGHT_WRIST].x, landmarks[mp_pose.PoseLandmark.RIGHT_WRIST].y]\n",
    "\n",
    "    # Dynamically determine front and back legs based on knee angle\n",
    "    left_knee_angle = calculate_angle(l_hip, l_knee, l_ankle)\n",
    "    right_knee_angle = calculate_angle(r_hip, r_knee, r_ankle)\n",
    "    \n",
    "    if left_knee_angle < right_knee_angle:  # Left leg is front leg\n",
    "        front_hip, front_knee, front_ankle = l_hip, l_knee, l_ankle\n",
    "        back_hip, back_knee, back_ankle = r_hip, r_knee, r_ankle\n",
    "        front_knee_angle = left_knee_angle\n",
    "        back_leg_angle = right_knee_angle\n",
    "        hip_angle = calculate_angle(l_hip, r_hip, [r_hip[0] + 1, r_hip[1]])  # Left to right (rightward)\n",
    "    else:  # Right leg is front leg\n",
    "        front_hip, front_knee, front_ankle = r_hip, r_knee, r_ankle\n",
    "        back_hip, back_knee, back_ankle = l_hip, l_knee, l_ankle\n",
    "        front_knee_angle = right_knee_angle\n",
    "        back_leg_angle = left_knee_angle\n",
    "        hip_angle = calculate_angle(r_hip, l_hip, [l_hip[0] - 1, l_hip[1]])  # Right to left (leftward)\n",
    "\n",
    "    # Calculate other angles\n",
    "    torso_angle = calculate_angle(\n",
    "        [(l_hip[0] + r_hip[0]) / 2, (l_hip[1] + r_hip[1]) / 2],\n",
    "        [(l_shoulder[0] + r_shoulder[0]) / 2, (l_shoulder[1] + r_shoulder[1]) / 2],\n",
    "        [0, 1]  # Vertical reference\n",
    "    )\n",
    "    l_arm_angle = calculate_angle(l_shoulder, l_elbow, l_wrist)\n",
    "    r_arm_angle = calculate_angle(r_shoulder, r_elbow, r_wrist)\n",
    "    shoulder_hip_angle = calculate_angle(l_shoulder, r_shoulder, r_hip)\n",
    "\n",
    "    # Check thresholds and collect errors\n",
    "    if not THRESHOLDS[\"front_knee_angle\"][0] <= front_knee_angle <= THRESHOLDS[\"front_knee_angle\"][1]:\n",
    "        errors.append(\"Bend your front knee more\" if front_knee_angle > 100 else \"Straighten your front knee slightly\")\n",
    "    if not THRESHOLDS[\"back_leg_angle\"][0] <= back_leg_angle <= THRESHOLDS[\"back_leg_angle\"][1]:\n",
    "        errors.append(\"Straighten your back leg\" if back_leg_angle < 160 else \"Relax your back leg slightly\")\n",
    "    \n",
    "    # Hip orientation check (dynamic based on front leg)\n",
    "    if not THRESHOLDS[\"hip_orientation\"][0] <= hip_angle <= THRESHOLDS[\"hip_orientation\"][1]:\n",
    "        if front_hip == r_hip:  # Right leg is front\n",
    "            if r_hip[1] > l_hip[1]:  # Right hip higher\n",
    "                errors.append(\"Level your hips; right hip is too high\")\n",
    "            else:  # Left hip higher\n",
    "                errors.append(\"Level your hips; left hip is too high\")\n",
    "        else:  # Left leg is front\n",
    "            if l_hip[1] > r_hip[1]:  # Left hip higher\n",
    "                errors.append(\"Level your hips; left hip is too high\")\n",
    "            else:  # Right hip higher\n",
    "                errors.append(\"Level your hips; right hip is too high\")\n",
    "\n",
    "\n",
    "    \n",
    "    if not THRESHOLDS[\"arm_angle\"][0] <= l_arm_angle <= THRESHOLDS[\"arm_angle\"][1] or \\\n",
    "       not THRESHOLDS[\"arm_angle\"][0] <= r_arm_angle <= THRESHOLDS[\"arm_angle\"][1]:\n",
    "        errors.append(\"Raise your arms to shoulder level\" if l_arm_angle < 170 or r_arm_angle < 170 else \"Extend your arms fully\")                \n",
    "\n",
    "    #if not THRESHOLDS[\"torso_angle\"][0] <= torso_angle <= THRESHOLDS[\"torso_angle\"][1]:\n",
    "    #    errors.append(\"Straighten your torso\" if torso_angle < 80 else \"Lean back slightly\")\n",
    "    #if not THRESHOLDS[\"shoulder_hip_alignment\"][0] <= shoulder_hip_angle <= THRESHOLDS[\"shoulder_hip_alignment\"][1]:\n",
    "    #    errors.append(\"Align your shoulders over your hips\")\n",
    "\n",
    "    return errors[:3]  # Return top 3 errors\n",
    "\n",
    "\n",
    "# Main loop\n",
    "cap = cv2.VideoCapture(0)\n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    # Process frame with MediaPipe\n",
    "    results = pose.process(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))\n",
    "    if results.pose_landmarks:\n",
    "        # Draw current skeleton\n",
    "        mp_drawing.draw_landmarks(frame, results.pose_landmarks, mp_pose.POSE_CONNECTIONS)\n",
    "\n",
    "        # Check pose and get errors\n",
    "        errors = check_warrior_pose(results.pose_landmarks.landmark)\n",
    "\n",
    "        # Display top 3 errors\n",
    "        for i, error in enumerate(errors):\n",
    "            cv2.putText(frame, error, (10, 30 + i * 30), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 0, 255), 2)\n",
    "\n",
    "    cv2.imshow(\"Warrior Pose Correction\", frame)\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n",
    "pose.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5a779cd1-dff6-4d32-a02a-750940eb649c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4f3e5f15-9892-43da-8b9e-b058a0d64ac3",
   "metadata": {},
   "source": [
    "# Final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4e50861f-b974-4a53-a26c-ac865ca49a4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a3c1849c-ad0e-4565-87ce-5cff11048923",
   "metadata": {},
   "outputs": [],
   "source": [
    "class WarriorPoseAnalyzer:\n",
    "    def __init__(self, record_seconds=10, fps=30):\n",
    "        # Initialize MediaPipe Pose\n",
    "        self.mp_pose = mp.solutions.pose\n",
    "        self.mp_drawing = mp.solutions.drawing_utils\n",
    "        self.pose = self.mp_pose.Pose()\n",
    "\n",
    "        # Define adjustable thresholds\n",
    "        self.THRESHOLDS = {\n",
    "            \"front_knee_angle\": (70, 120),\n",
    "            \"back_leg_angle\": (150, 180),\n",
    "            \"hip_orientation\": (0, 30),\n",
    "            \"torso_angle\": (0, 100),\n",
    "            \"arm_angle\": (160, 180),\n",
    "            \"shoulder_hip_alignment\": (0, 40)\n",
    "        }\n",
    "\n",
    "        # Recording settings\n",
    "        self.fps = fps\n",
    "        self.delay_frames = 3 * fps  # 3 seconds delay\n",
    "        self.record_frames = record_seconds * fps  # Number of frames to record\n",
    "        self.frame_count = 0\n",
    "        self.recording = False\n",
    "        self.report = {\n",
    "            \"good_form_frames\": 0,\n",
    "            \"error_counts\": defaultdict(int)  # Tracks each error's frequency\n",
    "        }\n",
    "\n",
    "    def calculate_angle(self, p1, p2, p3):\n",
    "        \"\"\"Calculate the angle between three points in degrees.\"\"\"\n",
    "        a = np.array(p1)\n",
    "        b = np.array(p2)\n",
    "        c = np.array(p3)\n",
    "        ba = a - b\n",
    "        bc = c - b\n",
    "        cosine_angle = np.dot(ba, bc) / (np.linalg.norm(ba) * np.linalg.norm(bc))\n",
    "        angle = np.arccos(cosine_angle) * 180 / np.pi\n",
    "        return angle\n",
    "\n",
    "    def check_warrior_pose(self, landmarks):\n",
    "        \"\"\"Analyze Warrior II pose and return top 3 errors.\"\"\"\n",
    "        errors = []\n",
    "\n",
    "        l_hip = [landmarks[self.mp_pose.PoseLandmark.LEFT_HIP].x, landmarks[self.mp_pose.PoseLandmark.LEFT_HIP].y]\n",
    "        r_hip = [landmarks[self.mp_pose.PoseLandmark.RIGHT_HIP].x, landmarks[self.mp_pose.PoseLandmark.RIGHT_HIP].y]\n",
    "        l_knee = [landmarks[self.mp_pose.PoseLandmark.LEFT_KNEE].x, landmarks[self.mp_pose.PoseLandmark.LEFT_KNEE].y]\n",
    "        r_knee = [landmarks[self.mp_pose.PoseLandmark.RIGHT_KNEE].x, landmarks[self.mp_pose.PoseLandmark.RIGHT_KNEE].y]\n",
    "        l_ankle = [landmarks[self.mp_pose.PoseLandmark.LEFT_ANKLE].x, landmarks[self.mp_pose.PoseLandmark.LEFT_ANKLE].y]\n",
    "        r_ankle = [landmarks[self.mp_pose.PoseLandmark.RIGHT_ANKLE].x, landmarks[self.mp_pose.PoseLandmark.RIGHT_ANKLE].y]\n",
    "        l_shoulder = [landmarks[self.mp_pose.PoseLandmark.LEFT_SHOULDER].x, landmarks[self.mp_pose.PoseLandmark.LEFT_SHOULDER].y]\n",
    "        r_shoulder = [landmarks[self.mp_pose.PoseLandmark.RIGHT_SHOULDER].x, landmarks[self.mp_pose.PoseLandmark.RIGHT_SHOULDER].y]\n",
    "        l_elbow = [landmarks[self.mp_pose.PoseLandmark.LEFT_ELBOW].x, landmarks[self.mp_pose.PoseLandmark.LEFT_ELBOW].y]\n",
    "        r_elbow = [landmarks[self.mp_pose.PoseLandmark.RIGHT_ELBOW].x, landmarks[self.mp_pose.PoseLandmark.RIGHT_ELBOW].y]\n",
    "        l_wrist = [landmarks[self.mp_pose.PoseLandmark.LEFT_WRIST].x, landmarks[self.mp_pose.PoseLandmark.LEFT_WRIST].y]\n",
    "        r_wrist = [landmarks[self.mp_pose.PoseLandmark.RIGHT_WRIST].x, landmarks[self.mp_pose.PoseLandmark.RIGHT_WRIST].y]\n",
    "\n",
    "        left_knee_angle = self.calculate_angle(l_hip, l_knee, l_ankle)\n",
    "        right_knee_angle = self.calculate_angle(r_hip, r_knee, r_ankle)\n",
    "        \n",
    "        if left_knee_angle < right_knee_angle:\n",
    "            front_hip, front_knee, front_ankle = l_hip, l_knee, l_ankle\n",
    "            back_hip, back_knee, back_ankle = r_hip, r_knee, r_ankle\n",
    "            front_knee_angle = left_knee_angle\n",
    "            back_leg_angle = right_knee_angle\n",
    "            hip_angle = self.calculate_angle(l_hip, r_hip, [r_hip[0] + 1, r_hip[1]])\n",
    "        else:\n",
    "            front_hip, front_knee, front_ankle = r_hip, r_knee, r_ankle\n",
    "            back_hip, back_knee, back_ankle = l_hip, l_knee, l_ankle\n",
    "            front_knee_angle = right_knee_angle\n",
    "            back_leg_angle = left_knee_angle\n",
    "            hip_angle = self.calculate_angle(r_hip, l_hip, [l_hip[0] - 1, l_hip[1]])\n",
    "\n",
    "        torso_angle = self.calculate_angle(\n",
    "            [(l_hip[0] + r_hip[0]) / 2, (l_hip[1] + r_hip[1]) / 2],\n",
    "            [(l_shoulder[0] + r_shoulder[0]) / 2, (l_shoulder[1] + r_shoulder[1]) / 2],\n",
    "            [0, 1]\n",
    "        )\n",
    "        l_arm_angle = self.calculate_angle(l_shoulder, l_elbow, l_wrist)\n",
    "        r_arm_angle = self.calculate_angle(r_shoulder, r_elbow, r_wrist)\n",
    "        shoulder_hip_angle = self.calculate_angle(l_shoulder, r_shoulder, r_hip)\n",
    "\n",
    "        if not self.THRESHOLDS[\"front_knee_angle\"][0] <= front_knee_angle <= self.THRESHOLDS[\"front_knee_angle\"][1]:\n",
    "            errors.append(\"Bend your front knee more\" if front_knee_angle > 100 else \"Straighten your front knee slightly\")\n",
    "        if not self.THRESHOLDS[\"back_leg_angle\"][0] <= back_leg_angle <= self.THRESHOLDS[\"back_leg_angle\"][1]:\n",
    "            errors.append(\"Straighten your back leg\" if back_leg_angle < 160 else \"Relax your back leg slightly\")\n",
    "        \n",
    "        if not self.THRESHOLDS[\"hip_orientation\"][0] <= hip_angle <= self.THRESHOLDS[\"hip_orientation\"][1]:\n",
    "            if front_hip == r_hip:\n",
    "                if r_hip[1] > l_hip[1]:\n",
    "                    errors.append(\"Level your hips; right hip is too high\")\n",
    "                else:\n",
    "                    errors.append(\"Level your hips; left hip is too high\")\n",
    "            else:\n",
    "                if l_hip[1] > r_hip[1]:\n",
    "                    errors.append(\"Level your hips; left hip is too high\")\n",
    "                else:\n",
    "                    errors.append(\"Level your hips; right hip is too high\")\n",
    "\n",
    "        if not self.THRESHOLDS[\"arm_angle\"][0] <= l_arm_angle <= self.THRESHOLDS[\"arm_angle\"][1] or \\\n",
    "           not self.THRESHOLDS[\"arm_angle\"][0] <= r_arm_angle <= self.THRESHOLDS[\"arm_angle\"][1]:\n",
    "            errors.append(\"Raise your arms to shoulder level\" if l_arm_angle < 170 or r_arm_angle < 170 else \"Extend your arms fully\")\n",
    "\n",
    "        return errors[:3]\n",
    "\n",
    "    def process_frame(self, frame):\n",
    "        \"\"\"Process a single frame and return the annotated frame with errors or correct form message.\"\"\"\n",
    "        results = self.pose.process(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))\n",
    "        \n",
    "        if results.pose_landmarks:\n",
    "            self.mp_drawing.draw_landmarks(frame, results.pose_landmarks, self.mp_pose.POSE_CONNECTIONS)\n",
    "            errors = self.check_warrior_pose(results.pose_landmarks.landmark)\n",
    "\n",
    "            # Update frame count and recording logic\n",
    "            self.frame_count += 1\n",
    "            if self.frame_count > self.delay_frames and not self.recording:\n",
    "                self.recording = True\n",
    "                self.start_frame = self.frame_count\n",
    "            if self.recording and (self.frame_count - self.start_frame) <= self.record_frames:\n",
    "                if not errors:\n",
    "                    self.report[\"good_form_frames\"] += 1\n",
    "                for error in errors:\n",
    "                    self.report[\"error_counts\"][error] += 1\n",
    "\n",
    "            # Display errors or \"Correct Form\"\n",
    "            if errors:\n",
    "                for i, error in enumerate(errors):\n",
    "                    cv2.putText(frame, error, (10, 30 + i * 30), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 0, 255), 2)\n",
    "            else:\n",
    "                cv2.putText(frame, \"Correct Form\", (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 0), 2)\n",
    "\n",
    "        return frame\n",
    "\n",
    "    def generate_report(self):\n",
    "        \"\"\"Generate and print an exercise report.\"\"\"\n",
    "        total_recorded_frames = self.record_frames\n",
    "        good_form_seconds = self.report[\"good_form_frames\"] / self.fps\n",
    "        total_seconds = total_recorded_frames / self.fps\n",
    "\n",
    "        print(\"\\n--- Warrior II Exercise Report ---\")\n",
    "        print(f\"Total Recorded Time: {total_seconds:.2f} seconds\")\n",
    "        print(f\"Good Form Duration: {good_form_seconds:.2f} seconds ({(good_form_seconds / total_seconds) * 100:.1f}%)\")\n",
    "        print(\"Errors Detected:\")\n",
    "        if self.report[\"error_counts\"]:\n",
    "            for error, count in self.report[\"error_counts\"].items():\n",
    "                error_seconds = count / self.fps\n",
    "                print(f\"  - '{error}': {count} frames ({error_seconds:.2f} seconds, {(count / total_recorded_frames) * 100:.1f}%)\")\n",
    "        else:\n",
    "            print(\"  - No errors detected!\")\n",
    "        print(\"--------------------------------\\n\")\n",
    "\n",
    "    def run(self):\n",
    "        \"\"\"Run the analyzer with webcam input.\"\"\"\n",
    "        cap = cv2.VideoCapture(0)\n",
    "        while cap.isOpened():\n",
    "            ret, frame = cap.read()\n",
    "            if not ret:\n",
    "                break\n",
    "\n",
    "            frame = self.process_frame(frame)\n",
    "            cv2.imshow(\"Warrior Pose Correction\", frame)\n",
    "\n",
    "            # Stop recording and generate report when time is up\n",
    "            if self.recording and (self.frame_count - self.start_frame) > self.record_frames:\n",
    "                self.generate_report()\n",
    "                break\n",
    "\n",
    "            if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "                if self.recording:\n",
    "                    self.generate_report()\n",
    "                break\n",
    "\n",
    "        cap.release()\n",
    "        cv2.destroyAllWindows()\n",
    "        self.pose.close()\n",
    "\n",
    "    def __del__(self):\n",
    "        self.pose.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a5d92cde-07b7-4073-8b2c-0283ac262d5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:5: SyntaxWarning: invalid escape sequence '\\D'\n",
      "<>:5: SyntaxWarning: invalid escape sequence '\\D'\n",
      "C:\\Users\\lenovo\\AppData\\Local\\Temp\\ipykernel_4376\\918094476.py:5: SyntaxWarning: invalid escape sequence '\\D'\n",
      "  analyzer.process_image(\"./New folder\\DATASET/TRAIN/warrior2/00000131.jpg\", \"output.jpg\")\n",
      "Exception ignored in: <function WarriorPoseAnalyzer.__del__ at 0x000001D569A1C7C0>\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\lenovo\\AppData\\Local\\Temp\\ipykernel_4376\\1102365766.py\", line 219, in __del__\n",
      "  File \"D:\\anaconda\\envs\\fypvis\\Lib\\site-packages\\mediapipe\\python\\solution_base.py\", line 361, in close\n",
      "    raise ValueError('Closing SolutionBase._graph which is already None')\n",
      "ValueError: Closing SolutionBase._graph which is already None\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed image saved to output.jpg\n"
     ]
    }
   ],
   "source": [
    "# Usage\n",
    "if __name__ == \"__main__\":\n",
    "    analyzer = WarriorPoseAnalyzer(record_seconds=10)  # Record for 10 seconds after 3-second delay\n",
    "    analyzer.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14187266-cafe-49ac-bb5f-bebf91974b4d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5509912-b3b6-4b75-950a-2140a10b0dc6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c02d095-efa2-411e-85b7-09eced8f4ee2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# backend/main.py\n",
    "from fastapi import FastAPI, WebSocket\n",
    "import cv2\n",
    "import asyncio\n",
    "import base64\n",
    "import numpy as np\n",
    "import websockets\n",
    "import json\n",
    "\n",
    "app = FastAPI()\n",
    "\n",
    "# Your existing exercise classes\n",
    "from exercise1 import Exercise1Analyzer\n",
    "from exercise2 import Exercise2Analyzer\n",
    "from exercise3 import Exercise3Analyzer\n",
    "from exercise4 import Exercise4Analyzer\n",
    "\n",
    "# Initialize analyzers\n",
    "analyzers = {\n",
    "    \"Squat\": SquatAnalyzer(),\n",
    "    \"exercise2\": Exercise2Analyzer(),\n",
    "    \"exercise3\": Exercise3Analyzer(),\n",
    "    \"exercise4\": Exercise4Analyzer()\n",
    "}\n",
    "\n",
    "class VideoProcessor:\n",
    "    def __init__(self):\n",
    "        self.cap = None\n",
    "        self.is_running = False\n",
    "        self.current_exercise = None\n",
    "\n",
    "    async def process_video(self, websocket):\n",
    "        if not self.cap:\n",
    "            self.cap = cv2.VideoCapture(0)\n",
    "        \n",
    "        while self.is_running and self.current_exercise:\n",
    "            ret, frame = self.cap.read()\n",
    "            if not ret:\n",
    "                break\n",
    "\n",
    "            analyzer = analyzers[self.current_exercise]\n",
    "            processed_frame, metrics = analyzer.process_frame(frame)\n",
    "            \n",
    "            _, buffer = cv2.imencode('.jpg', processed_frame)\n",
    "            frame_base64 = base64.b64encode(buffer).decode('utf-8')\n",
    "            \n",
    "            await websocket.send(json.dumps({\n",
    "                \"frame\": frame_base64,\n",
    "                \"metrics\": metrics\n",
    "            }))\n",
    "            await asyncio.sleep(0.033)\n",
    "\n",
    "    def stop(self):\n",
    "        self.is_running = False\n",
    "        if self.cap:\n",
    "            self.cap.release()\n",
    "\n",
    "video_processor = VideoProcessor()\n",
    "\n",
    "async def websocket_handler(websocket):\n",
    "    try:\n",
    "        while True:\n",
    "            data = json.loads(await websocket.recv())\n",
    "            exercise = data.get(\"exercise\")\n",
    "            \n",
    "            if exercise == \"stop\":\n",
    "                video_processor.stop()\n",
    "            elif exercise in analyzers:\n",
    "                video_processor.current_exercise = exercise\n",
    "                if not video_processor.is_running:\n",
    "                    video_processor.is_running = True\n",
    "                    asyncio.create_task(video_processor.process_video(websocket))\n",
    "                    \n",
    "    except websockets.ConnectionClosed:\n",
    "        video_processor.stop()\n",
    "    except Exception as e:\n",
    "        video_processor.stop()\n",
    "\n",
    "async def main():\n",
    "    async with websockets.serve(\n",
    "        websocket_handler,\n",
    "        \"localhost\",\n",
    "        8000\n",
    "    ):\n",
    "        await asyncio.Future()  # Run forever\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    asyncio.run(main())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53a74937-8e00-4340-8703-389f91163fce",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e082775-9536-4f94-931c-769c5d917082",
   "metadata": {},
   "outputs": [],
   "source": [
    "# backend/main.py\n",
    "import cv2\n",
    "import asyncio\n",
    "import json\n",
    "import logging\n",
    "import threading\n",
    "import time\n",
    "import websockets\n",
    "from functools import partial\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Import your analyzer classes\n",
    "\"\"\"\"\"\n",
    "IMPORTANT, ADD PATH\n",
    "\"\"\"\"\"\n",
    "from squats_analyzer import SquatsAnalyzer\n",
    "\n",
    "class VideoServer:\n",
    "    def __init__(self):\n",
    "        self.cap = None\n",
    "        self.clients = set()\n",
    "        self.event_loop = None\n",
    "        self.server = None\n",
    "        self.running = False\n",
    "        self.current_analyzer = None\n",
    "\n",
    "        # Initialize analyzers with a reference to the broadcast method\n",
    "        self.analyzers = {\n",
    "            \"Squats\": SquatsAnalyzer(),\n",
    "        }\n",
    "\n",
    "    async def process_frames(self, input_source=0):\n",
    "        \"\"\"Centralized frame processing loop.\"\"\"\n",
    "        self.cap = cv2.VideoCapture(input_source)\n",
    "        if not self.cap.isOpened():\n",
    "            logger.error(f\"Error: Could not open video source {input_source}\")\n",
    "            return\n",
    "\n",
    "        try:\n",
    "            while self.running and self.cap.isOpened():\n",
    "                start_time = time.time()\n",
    "\n",
    "                success, frame = self.cap.read()\n",
    "                if not success:\n",
    "                    logger.info(\"End of video\")\n",
    "                    break\n",
    "\n",
    "                # Process frame with the current analyzer if selected\n",
    "                if self.current_analyzer:\n",
    "                    processed_data = await self.current_analyzer.process_video(frame)\n",
    "                    if processed_data:\n",
    "                        await self._broadcast(processed_data)\n",
    "\n",
    "                # Maintain ~30 FPS\n",
    "                processing_time = time.time() - start_time\n",
    "                await asyncio.sleep(max(0, 0.03 - processing_time))\n",
    "\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error during frame processing: {e}\")\n",
    "        finally:\n",
    "            if self.cap:\n",
    "                self.cap.release()\n",
    "            if self.current_analyzer:\n",
    "                report = self.current_analyzer.generate_report()\n",
    "                print(\"\\n\" + report)\n",
    "                with open('report.txt', 'w') as f:\n",
    "                    f.write(report)\n",
    "            logger.info(\"Video processing stopped\")\n",
    "\n",
    "    async def _broadcast(self, message):\n",
    "        \"\"\"Broadcast a message to all connected clients.\"\"\"\n",
    "        if not self.clients:\n",
    "            return\n",
    "        message_json = json.dumps(message)\n",
    "        await asyncio.gather(\n",
    "            *[client.send(message_json) for client in self.clients],\n",
    "            return_exceptions=True\n",
    "        )\n",
    "\n",
    "    def broadcast_message(self, message):\n",
    "        \"\"\"Broadcast a message to all connected clients from any thread.\"\"\"\n",
    "        if self.event_loop and self.clients:\n",
    "            asyncio.run_coroutine_threadsafe(self._broadcast(message), self.event_loop)\n",
    "\n",
    "    async def websocket_handler(self, websocket):\n",
    "        self.clients.add(websocket)\n",
    "        logger.info(f\"New client connected: {websocket.remote_address}\")\n",
    "        try:\n",
    "            async for message in websocket:\n",
    "                try:\n",
    "                    data = json.loads(message)\n",
    "                    action = data.get('action')\n",
    "                    exercise = data.get('exercise')\n",
    "                    logger.info(f\"Received action: {action}, exercise: {exercise}\")\n",
    "\n",
    "                    if action == 'connect':\n",
    "                        await websocket.send(json.dumps({\"status\": \"connected\"}))\n",
    "                    elif action == 'start':\n",
    "                        if exercise in self.analyzers:\n",
    "                            if not self.running:\n",
    "                                self.current_analyzer = self.analyzers[exercise]\n",
    "                                self.current_analyzer.reset_counters()  # Reset counters for new exercise\n",
    "                                self.running = True\n",
    "                                asyncio.create_task(self.process_frames())\n",
    "                                await websocket.send(json.dumps({\"status\": \"started\", \"exercise\": exercise}))\n",
    "                            else:\n",
    "                                await websocket.send(json.dumps({\"status\": \"already_running\"}))\n",
    "                        else:\n",
    "                            await websocket.send(json.dumps({\"error\": \"Invalid exercise\"}))\n",
    "                    elif action == 'stop':\n",
    "                        if self.running:\n",
    "                            self.running = False\n",
    "                            await websocket.send(json.dumps({\"status\": \"stopped\"}))\n",
    "                        else:\n",
    "                            await websocket.send(json.dumps({\"status\": \"not_running\"}))\n",
    "                    else:\n",
    "                        logger.warning(f\"Unknown action: {action}\")\n",
    "                        await websocket.send(json.dumps({\"error\": \"Unknown action\"}))\n",
    "                except json.JSONDecodeError:\n",
    "                    logger.error(\"Invalid JSON received\")\n",
    "                    await websocket.send(json.dumps({\"error\": \"Invalid request format\"}))\n",
    "        except websockets.ConnectionClosed:\n",
    "            logger.info(f\"Client disconnected: {websocket.remote_address}\")\n",
    "        finally:\n",
    "            self.clients.remove(websocket)\n",
    "\n",
    "    def start_server(self, host='localhost', port=8000):\n",
    "        \"\"\"Start the WebSocket server.\"\"\"\n",
    "        def run_event_loop(loop):\n",
    "            asyncio.set_event_loop(loop)\n",
    "            loop.run_forever()\n",
    "\n",
    "        self.event_loop = asyncio.new_event_loop()\n",
    "        server_thread = threading.Thread(\n",
    "            target=run_event_loop,\n",
    "            args=(self.event_loop,),\n",
    "            daemon=True\n",
    "        )\n",
    "        server_thread.start()\n",
    "\n",
    "        asyncio.run_coroutine_threadsafe(self._start_websocket_server(host, port), self.event_loop)\n",
    "        logger.info(f\"WebSocket server started on ws://{host}:{port}\")\n",
    "\n",
    "    async def _start_websocket_server(self, host, port):\n",
    "        self.server = await websockets.serve(\n",
    "            partial(self.websocket_handler),\n",
    "            host,\n",
    "            port,\n",
    "            ping_interval=30\n",
    "        )\n",
    "\n",
    "    def stop_server(self):\n",
    "        \"\"\"Stop the WebSocket server.\"\"\"\n",
    "        self.running = False\n",
    "        if self.server:\n",
    "            self.server.close()\n",
    "            asyncio.run_coroutine_threadsafe(self.server.wait_closed(), self.event_loop)\n",
    "            self.event_loop.call_soon_threadsafe(self.event_loop.stop)\n",
    "            logger.info(\"WebSocket server stopped\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    server = VideoServer()\n",
    "    server.start_server()\n",
    "    try:\n",
    "        while True:\n",
    "            time.sleep(1)  # Keep main thread alive\n",
    "    except KeyboardInterrupt:\n",
    "        server.stop_server()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "928c8a73-5c75-407e-aaa5-e6b715f4d01f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b270e13-dd92-42e9-ad31-963e07f29898",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.signal import savgol_filter\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from fastapi import FastAPI, WebSocket, WebSocketDisconnect\n",
    "from fastapi.middleware.cors import CORSMiddleware\n",
    "import cv2\n",
    "import tensorflow as tf\n",
    "import threading\n",
    "import base64\n",
    "import os\n",
    "import numpy as np\n",
    "import asyncio\n",
    "import json\n",
    "from collections import deque\n",
    "import time\n",
    "import websockets\n",
    "import base64\n",
    "import time\n",
    "from typing import Dict, List\n",
    "import joblib\n",
    "from tensorflow.keras.models import load_model\n",
    "import logging\n",
    "import mediapipe as mp\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import asyncio\n",
    "import threading\n",
    "import time\n",
    "import base64\n",
    "import logging\n",
    "import websockets\n",
    "from functools import partial\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "\n",
    "class SquatAnalyzer:\n",
    "    def __init__(self, model_path = r\"C:\\Users\\Lenovo\\Desktop\\FYYP_MID EVAL\\Website_PhysioVision\\PhysioVision\\Backend_Vision\\models_vision\\best_squat_model.keras\", scaler_path = r\"C:\\Users\\Lenovo\\Desktop\\FYYP_MID EVAL\\Website_PhysioVision\\PhysioVision\\Backend_Vision\\models_vision\\preprocessed_data_scaler.joblib\", label_encoder_path = r\"C:\\Users\\Lenovo\\Desktop\\FYYP_MID EVAL\\Website_PhysioVision\\PhysioVision\\Backend_Vision\\models_vision\\preprocessed_data_label_encoder.joblib\" , window_size=30):\n",
    "        \"\"\"Initialize the squat analyzer with trained model and preprocessing tools\"\"\"\n",
    "        # Load model and preprocessing tools\n",
    "        self.clients = set()\n",
    "        self.running = False\n",
    "        self.event_loop = None\n",
    "        self.server = None\n",
    "        self.capture = None\n",
    "        self.detector_thread = None\n",
    "\n",
    "        self.model = tf.keras.models.load_model(model_path)\n",
    "        self.scaler = joblib.load(scaler_path)\n",
    "        self.label_encoder = joblib.load(label_encoder_path)\n",
    "        self.window_size = window_size\n",
    "        \n",
    "        # Initialize MediaPipe Pose\n",
    "        self.mp_pose = mp.solutions.pose\n",
    "        self.pose = self.mp_pose.Pose(\n",
    "            min_detection_confidence=0.5,\n",
    "            min_tracking_confidence=0.5,\n",
    "            model_complexity=1\n",
    "        )\n",
    "        self.mp_drawing = mp.solutions.drawing_utils\n",
    "        self.mp_drawing_styles = mp.solutions.drawing_styles\n",
    "        \n",
    "        # Create buffer for storing features\n",
    "        self.features_buffer = deque(maxlen=window_size)\n",
    "        \n",
    "        # Store current prediction and confidence\n",
    "        self.current_prediction = None\n",
    "        self.prediction_confidence = 0.0\n",
    "        self.last_predictions = deque(maxlen=5)  # Store last 5 predictions for smoothing\n",
    "        \n",
    "        \n",
    "        # Feature names for the processed angles\n",
    "        self.feature_names = [\n",
    "            'left_knee_angle', 'right_knee_angle', \n",
    "            'left_hip_angle', 'right_hip_angle',\n",
    "            'torso_vertical_angle', 'head_torso_angle',\n",
    "            'knee_distance_normalized', 'ankle_distance_normalized',\n",
    "            'left_squat_depth', 'right_squat_depth',\n",
    "            'left_knee_angle_velocity', 'right_knee_angle_velocity',\n",
    "            'left_hip_angle_velocity', 'right_hip_angle_velocity',\n",
    "            'torso_vertical_angle_velocity', 'head_torso_angle_velocity',\n",
    "            'knee_distance_normalized_velocity', 'ankle_distance_normalized_velocity',\n",
    "            'left_squat_depth_velocity', 'right_squat_depth_velocity'\n",
    "        ]\n",
    "        \n",
    "        # Keypoint mapping from MediaPipe to our preprocessing format\n",
    "        self.keypoint_map = {\n",
    "            'NOSE': self.mp_pose.PoseLandmark.NOSE,\n",
    "            'LEFT_SHOULDER': self.mp_pose.PoseLandmark.LEFT_SHOULDER,\n",
    "            'RIGHT_SHOULDER': self.mp_pose.PoseLandmark.RIGHT_SHOULDER,\n",
    "            'LEFT_HIP': self.mp_pose.PoseLandmark.LEFT_HIP,\n",
    "            'RIGHT_HIP': self.mp_pose.PoseLandmark.RIGHT_HIP,\n",
    "            'LEFT_KNEE': self.mp_pose.PoseLandmark.LEFT_KNEE,\n",
    "            'RIGHT_KNEE': self.mp_pose.PoseLandmark.RIGHT_KNEE,\n",
    "            'LEFT_ANKLE': self.mp_pose.PoseLandmark.LEFT_ANKLE,\n",
    "            'RIGHT_ANKLE': self.mp_pose.PoseLandmark.RIGHT_ANKLE\n",
    "        }\n",
    "        \n",
    "        # Error explanations\n",
    "        self.error_explanations = {\n",
    "            'bad_back_round': \"Your back is rounding. Keep your spine neutral.\",\n",
    "            'bad_back_warp': \"Your back is excessively arched. Maintain a neutral spine.\",\n",
    "            'bad_head': \"Head position incorrect. Look slightly downward, keeping your neck aligned with your spine.\",\n",
    "            'bad_inner_thigh': \"Knees collapsing inward. Keep knees aligned with toes.\",\n",
    "            'bad_shallow': \"Squat is too shallow. Try to go deeper with proper form.\",\n",
    "            'bad_toe': \"Foot positioning issue. Keep feet shoulder-width apart with toes slightly turned out.\",\n",
    "            'good': \"Good form! Keep it up.\"\n",
    "        }\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # Add rep counting variables\n",
    "        self.rep_count = 0\n",
    "        self.state = 'STANDING'  # Initial state\n",
    "        self.depth_history = deque(maxlen=10)  # Store recent squat depths for dynamic thresholding\n",
    "        self.min_depth = None  # Dynamic minimum depth (updated during exercise)\n",
    "        self.max_depth = None  # Dynamic maximum depth (updated during exercise)\n",
    "        self.depth_threshold_factor = 0.5  # Percentage of depth range to consider a state change\n",
    "\n",
    "        # Add error occurrence tracking\n",
    "        self.error_counts = {\n",
    "            'bad_back_round': 0,\n",
    "            'bad_back_warp': 0,\n",
    "            'bad_head': 0,\n",
    "            'bad_inner_thigh': 0,\n",
    "            'bad_shallow': 0,\n",
    "            'bad_toe': 0,\n",
    "            'good': 0  # Track good form too\n",
    "        }\n",
    "\n",
    "\n",
    "\n",
    "        \n",
    "        \n",
    "        print(\"Squat Analyzer initialized successfully\")\n",
    "\n",
    "    def _landmarks_to_keypoints_dict(self, landmarks):\n",
    "        \"\"\"Convert MediaPipe landmarks to format compatible with preprocessing code\"\"\"\n",
    "        keypoints = {}\n",
    "        for name, landmark_id in self.keypoint_map.items():\n",
    "            landmark = landmarks.landmark[landmark_id]\n",
    "            keypoints[f'{name}_x'] = landmark.x\n",
    "            keypoints[f'{name}_y'] = landmark.y\n",
    "            keypoints[f'{name}_z'] = landmark.z\n",
    "        return keypoints\n",
    "\n",
    "    def _keypoints_dict_to_df(self, keypoints_dict):\n",
    "        \"\"\"Convert keypoints dictionary to DataFrame\"\"\"\n",
    "        return pd.DataFrame([keypoints_dict])\n",
    "\n",
    "    def _calculate_vector(self, df, point1, point2):\n",
    "        \"\"\"Calculate vector between two keypoints\"\"\"\n",
    "        vec = np.zeros((len(df), 3))\n",
    "        vec[:, 0] = df[f'{point2}_x'].values - df[f'{point1}_x'].values\n",
    "        vec[:, 1] = df[f'{point2}_y'].values - df[f'{point1}_y'].values\n",
    "        vec[:, 2] = df[f'{point2}_z'].values - df[f'{point1}_z'].values\n",
    "        return vec\n",
    "\n",
    "    def _normalize_vector(self, vec):\n",
    "        \"\"\"Normalize a vector to unit length\"\"\"\n",
    "        magnitude = np.sqrt(np.sum(vec**2, axis=1))\n",
    "        magnitude = np.where(magnitude == 0, 1e-10, magnitude)\n",
    "        vec_normalized = vec / magnitude[:, np.newaxis]\n",
    "        return vec_normalized\n",
    "\n",
    "    def _angle_between_vectors(self, vec1, vec2):\n",
    "        \"\"\"Calculate the angle between two 3D vectors in degrees\"\"\"\n",
    "        vec1_norm = self._normalize_vector(vec1)\n",
    "        vec2_norm = self._normalize_vector(vec2)\n",
    "        dot_product = np.sum(vec1_norm * vec2_norm, axis=1)\n",
    "        dot_product = np.clip(dot_product, -1.0, 1.0)\n",
    "        angles = np.degrees(np.arccos(dot_product))\n",
    "        return angles\n",
    "\n",
    "    def _extract_anatomical_angles(self, df):\n",
    "        \"\"\"Extract biomechanically relevant angles from keypoints\"\"\"\n",
    "        angles_df = pd.DataFrame()\n",
    "        \n",
    "        # Calculate vectors\n",
    "        shoulder_to_hip_left = self._calculate_vector(df, 'LEFT_SHOULDER', 'LEFT_HIP')\n",
    "        shoulder_to_hip_right = self._calculate_vector(df, 'RIGHT_SHOULDER', 'RIGHT_HIP')\n",
    "        hip_to_knee_left = self._calculate_vector(df, 'LEFT_HIP', 'LEFT_KNEE')\n",
    "        hip_to_knee_right = self._calculate_vector(df, 'RIGHT_HIP', 'RIGHT_KNEE')\n",
    "        knee_to_ankle_left = self._calculate_vector(df, 'LEFT_KNEE', 'LEFT_ANKLE')\n",
    "        knee_to_ankle_right = self._calculate_vector(df, 'RIGHT_KNEE', 'RIGHT_ANKLE')\n",
    "        nose_to_shoulder_mid = self._calculate_vector(df, 'NOSE', 'LEFT_SHOULDER')\n",
    "        \n",
    "        # Calculate angles\n",
    "        angles_df['left_knee_angle'] = self._angle_between_vectors(hip_to_knee_left, knee_to_ankle_left)\n",
    "        angles_df['right_knee_angle'] = self._angle_between_vectors(hip_to_knee_right, knee_to_ankle_right)\n",
    "        angles_df['left_hip_angle'] = self._angle_between_vectors(shoulder_to_hip_left, hip_to_knee_left)\n",
    "        angles_df['right_hip_angle'] = self._angle_between_vectors(shoulder_to_hip_right, hip_to_knee_right)\n",
    "        \n",
    "        # Back angles (relative to vertical)\n",
    "        vertical = np.zeros_like(shoulder_to_hip_left)\n",
    "        vertical[:, 1] = 1  # Y axis is usually vertical in pose estimation\n",
    "        angles_df['torso_vertical_angle'] = self._angle_between_vectors(shoulder_to_hip_left, vertical)\n",
    "        \n",
    "        # Head angle relative to torso\n",
    "        angles_df['head_torso_angle'] = self._angle_between_vectors(nose_to_shoulder_mid, shoulder_to_hip_left)\n",
    "        \n",
    "        # Knee distance (for detecting knee valgus/varus)\n",
    "        hip_width = np.sqrt(\n",
    "            (df['RIGHT_HIP_x'] - df['LEFT_HIP_x'])**2 + \n",
    "            (df['RIGHT_HIP_z'] - df['LEFT_HIP_z'])**2\n",
    "        )\n",
    "        knee_distance = np.sqrt(\n",
    "            (df['RIGHT_KNEE_x'] - df['LEFT_KNEE_x'])**2 + \n",
    "            (df['RIGHT_KNEE_z'] - df['LEFT_KNEE_z'])**2\n",
    "        )\n",
    "        angles_df['knee_distance_normalized'] = knee_distance / hip_width\n",
    "        \n",
    "        # Foot positioning\n",
    "        ankle_distance = np.sqrt(\n",
    "            (df['RIGHT_ANKLE_x'] - df['LEFT_ANKLE_x'])**2 + \n",
    "            (df['RIGHT_ANKLE_z'] - df['LEFT_ANKLE_z'])**2\n",
    "        )\n",
    "        angles_df['ankle_distance_normalized'] = ankle_distance / hip_width\n",
    "        \n",
    "        # Squat depth - hip height relative to knee height\n",
    "        angles_df['left_squat_depth'] = df['LEFT_HIP_y'] - df['LEFT_KNEE_y']\n",
    "        angles_df['right_squat_depth'] = df['RIGHT_HIP_y'] - df['RIGHT_KNEE_y']\n",
    "        \n",
    "        return angles_df\n",
    "\n",
    "    def _add_velocity_features(self, current_features, prev_features=None, fps=30):\n",
    "        \"\"\"Add velocity features based on frame-to-frame changes\"\"\"\n",
    "        velocity_features = {}\n",
    "        \n",
    "        if prev_features is None:\n",
    "            # No previous frame, set velocities to 0\n",
    "            for column in current_features.index:\n",
    "                velocity_features[f'{column}_velocity'] = 0\n",
    "        else:\n",
    "            # Calculate frame-to-frame changes\n",
    "            for column in current_features.index:\n",
    "                velocity_features[f'{column}_velocity'] = (current_features[column] - prev_features[column]) * fps\n",
    "                \n",
    "        # Combine with current features\n",
    "        all_features = {}\n",
    "        for column in current_features.index:\n",
    "            all_features[column] = current_features[column]\n",
    "        all_features.update(velocity_features)\n",
    "        \n",
    "        return pd.Series(all_features)\n",
    "\n",
    "    def _process_frame(self, frame):\n",
    "        \"\"\"Process a single frame and extract features\"\"\"\n",
    "        # Convert to RGB for MediaPipe\n",
    "        image_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        \n",
    "        # Process the image\n",
    "        results = self.pose.process(image_rgb)\n",
    "        \n",
    "        # If no pose detected, return None\n",
    "        if not results.pose_landmarks:\n",
    "            return None, frame\n",
    "        \n",
    "        # Draw pose landmarks on the image\n",
    "        annotated_image = frame.copy()\n",
    "        self.mp_drawing.draw_landmarks(\n",
    "            annotated_image,\n",
    "            results.pose_landmarks,\n",
    "            self.mp_pose.POSE_CONNECTIONS,\n",
    "            landmark_drawing_spec=self.mp_drawing_styles.get_default_pose_landmarks_style()\n",
    "        )\n",
    "        \n",
    "        # Convert landmarks to keypoints dictionary\n",
    "        keypoints_dict = self._landmarks_to_keypoints_dict(results.pose_landmarks)\n",
    "        \n",
    "        # Convert to DataFrame\n",
    "        keypoints_df = self._keypoints_dict_to_df(keypoints_dict)\n",
    "        \n",
    "        # Extract angles\n",
    "        angles_df = self._extract_anatomical_angles(keypoints_df)\n",
    "        \n",
    "        # Get the first row as a Series\n",
    "        feature_series = angles_df.iloc[0]\n",
    "        \n",
    "        # Add velocity features if we have previous features\n",
    "        if len(self.features_buffer) > 0:\n",
    "            prev_features = self.features_buffer[-1]\n",
    "            feature_series = self._add_velocity_features(feature_series, prev_features)\n",
    "        else:\n",
    "            # No previous features, add zero velocities\n",
    "            feature_series = self._add_velocity_features(feature_series)\n",
    "        \n",
    "        # Add to buffer\n",
    "        self.features_buffer.append(feature_series)\n",
    "        \n",
    "        return feature_series, annotated_image\n",
    "\n",
    "    def _make_prediction(self):\n",
    "        \"\"\"Make a prediction using the current feature buffer\"\"\"\n",
    "        if len(self.features_buffer) < self.window_size:\n",
    "            return None, 0.0\n",
    "        \n",
    "        # Prepare features for model input\n",
    "        features_array = np.array([feature[self.feature_names] for feature in self.features_buffer])\n",
    "        \n",
    "        # Apply scaler transformation (same as in training)\n",
    "        normalized_features = self.scaler.transform(features_array)\n",
    "        \n",
    "        # Reshape for model input [batch_size=1, window_size, num_features]\n",
    "        model_input = normalized_features.reshape(1, self.window_size, len(self.feature_names))\n",
    "        \n",
    "        # Get prediction\n",
    "        prediction_probs = self.model.predict(model_input, verbose=0)[0]\n",
    "        predicted_class_idx = np.argmax(prediction_probs)\n",
    "        confidence = prediction_probs[predicted_class_idx]\n",
    "        \n",
    "        # Get class name\n",
    "        predicted_class = self.label_encoder.classes_[predicted_class_idx]\n",
    "        \n",
    "        return predicted_class, confidence\n",
    "\n",
    "    def _smooth_predictions(self, new_prediction, new_confidence):\n",
    "        \"\"\"Smooth predictions to avoid flickering\"\"\"\n",
    "        if new_prediction is None:\n",
    "            return self.current_prediction, self.prediction_confidence\n",
    "        \n",
    "        # Add to recent predictions\n",
    "        self.last_predictions.append((new_prediction, new_confidence))\n",
    "        \n",
    "        # Count occurrences of each prediction\n",
    "        prediction_counts = {}\n",
    "        total_confidence = {}\n",
    "        \n",
    "        for pred, conf in self.last_predictions:\n",
    "            if pred not in prediction_counts:\n",
    "                prediction_counts[pred] = 0\n",
    "                total_confidence[pred] = 0\n",
    "            \n",
    "            prediction_counts[pred] += 1\n",
    "            total_confidence[pred] += conf\n",
    "        \n",
    "        # Get the most common prediction\n",
    "        if prediction_counts:\n",
    "            most_common = max(prediction_counts.items(), key=lambda x: x[1])[0]\n",
    "            avg_confidence = total_confidence[most_common] / prediction_counts[most_common]\n",
    "            return most_common, avg_confidence\n",
    "        \n",
    "        return None, 0.0\n",
    "\n",
    "\n",
    "\n",
    "    def _update_rep_count(self, current_depth):\n",
    "        \"\"\"Update squat state and count reps based on squat depth\"\"\"\n",
    "        # Use average of left and right squat depth for consistency\n",
    "        avg_depth = (current_depth['left_squat_depth'] + current_depth['right_squat_depth']) / 2\n",
    "        self.depth_history.append(avg_depth)\n",
    "\n",
    "        # Update min and max depths dynamically\n",
    "        if len(self.depth_history) == self.depth_history.maxlen:\n",
    "            current_min = min(self.depth_history)\n",
    "            current_max = max(self.depth_history)\n",
    "            \n",
    "            if self.min_depth is None or current_min < self.min_depth:\n",
    "                self.min_depth = current_min\n",
    "            if self.max_depth is None or current_max > self.max_depth:\n",
    "                self.max_depth = current_max\n",
    "\n",
    "            # Calculate dynamic threshold\n",
    "            if self.min_depth is not None and self.max_depth is not None:\n",
    "                depth_range = self.max_depth - self.min_depth\n",
    "                threshold = self.min_depth + (depth_range * self.depth_threshold_factor)\n",
    "\n",
    "                # State transitions\n",
    "                if self.state == 'STANDING' and avg_depth < threshold:\n",
    "                    self.state = 'SQUATTING'\n",
    "                elif self.state == 'SQUATTING' and avg_depth > threshold:\n",
    "                    self.state = 'STANDING'\n",
    "                    self.rep_count += 1  # Count a rep when returning to standing\n",
    "\n",
    "    def _update_error_counts(self, prediction):\n",
    "        \"\"\"Update the count of the current prediction/error\"\"\"\n",
    "        if prediction in self.error_counts:\n",
    "            self.error_counts[prediction] += 1\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "    def draw_feedback(self, image, prediction, confidence):\n",
    "        h, w, _ = image.shape\n",
    "        \n",
    "        # Background for text (make it larger to fit rep counter)\n",
    "        cv2.rectangle(image, (0, h-140), (w, h), (0, 0, 0), -1)\n",
    "        \n",
    "        # Draw form feedback (existing code)\n",
    "        if prediction is None:\n",
    "            cv2.putText(image, \"Getting ready...\", (10, h-100), \n",
    "                        cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2)\n",
    "        else:\n",
    "            # Determine text color based on prediction\n",
    "            if prediction == 'good':\n",
    "                text_color = (0, 255, 0)  # Green for good form\n",
    "            else:\n",
    "                text_color = (0, 0, 255)  # Red for errors\n",
    "            \n",
    "            # Add prediction and confidence\n",
    "            cv2.putText(image, f\"Form: {prediction}\", (10, h-100), \n",
    "                        cv2.FONT_HERSHEY_SIMPLEX, 1, text_color, 2)\n",
    "            cv2.putText(image, f\"Confidence: {confidence:.2f}\", (10, h-70), \n",
    "                        cv2.FONT_HERSHEY_SIMPLEX, 1, text_color, 2)\n",
    "            \n",
    "            # Add explanation for errors\n",
    "            if prediction in self.error_explanations:\n",
    "                explanation = self.error_explanations[prediction]\n",
    "                cv2.putText(image, explanation, (w//4, 30), \n",
    "                            cv2.FONT_HERSHEY_SIMPLEX, 0.8, text_color, 2)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # Add rep count display\n",
    "        cv2.putText(image, f\"Reps: {self.rep_count}\", (10, h-40),\n",
    "                    cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2)\n",
    "        \n",
    "        return image\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def generate_report(self):\n",
    "        \"\"\"Generate a report summarizing reps and error occurrences\"\"\"\n",
    "        report = f\"Squat Analysis Report - {time.strftime('%Y-%m-%d %H:%M:%S')}\\n\"\n",
    "        report += \"=\" * 50 + \"\\n\"\n",
    "        report += f\"Total Reps Performed: {self.rep_count}\\n\\n\"\n",
    "        report += \"Form Analysis:\\n\"\n",
    "        report += \"-\" * 20 + \"\\n\"\n",
    "        \n",
    "        total_frames_with_prediction = sum(self.error_counts.values())\n",
    "        if total_frames_with_prediction > 0:\n",
    "            for error, count in self.error_counts.items():\n",
    "                percentage = (count / total_frames_with_prediction) * 100\n",
    "                explanation = self.error_explanations.get(error, \"No explanation available\")\n",
    "                report += f\"{error}: {count} occurrences ({percentage:.1f}%)\\n\"\n",
    "                report += f\"  - {explanation}\\n\"\n",
    "        else:\n",
    "            report += \"No form predictions recorded.\\n\"\n",
    "        \n",
    "        report += \"=\" * 50\n",
    "        return report\n",
    "\n",
    "    def reset_counters(self):\n",
    "        \"\"\"Reset rep count and error counts\"\"\"\n",
    "        self.rep_count = 0\n",
    "        self.state = 'STANDING'\n",
    "        self.depth_history.clear()\n",
    "        self.min_depth = None\n",
    "        self.max_depth = None\n",
    "        for error in self.error_counts:\n",
    "            self.error_counts[error] = 0\n",
    "        print(\"Counters reset\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        \n",
    "\n",
    "\n",
    "    def rescale_frame(self, frame, scale_percent=50):\n",
    "        \"\"\"\n",
    "        Rescale the input frame to improve processing speed.\n",
    "        \n",
    "        Args:\n",
    "            frame: Input frame to be rescaled\n",
    "            scale_percent: Percentage of original size (default: 50%)\n",
    "            \n",
    "        Returns:\n",
    "            Rescaled frame\n",
    "        \"\"\"\n",
    "        width = int(frame.shape[1] * scale_percent / 100)\n",
    "        height = int(frame.shape[0] * scale_percent / 100)\n",
    "        dim = (width, height)\n",
    "        \n",
    "        # Resize image\n",
    "        resized = cv2.resize(frame, dim, interpolation=cv2.INTER_AREA)\n",
    "        return resized\n",
    "\n",
    "\n",
    "\n",
    "   \n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Create analyzer instance\n",
    "    squatanalyzer = SquatAnalyzer()\n",
    "\n",
    "    try:\n",
    "        # Start WebSocket server\n",
    "        start_server(host='localhost', port=8765)\n",
    "\n",
    "        # Keep the main thread alive\n",
    "        while True:\n",
    "            time.sleep(1)\n",
    "    except KeyboardInterrupt:\n",
    "        analyzer.stop_server()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b77bde9-1372-441f-8b96-6797c03ed23d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "        \n",
    "    async def process_video(self, input_source=0):\n",
    "        cap = cv2.VideoCapture(input_source)\n",
    "        # we can remove this if we want to keep counters after every time the camera is opened. \n",
    "        self.reset_counters()\n",
    "        if not cap.isOpened():\n",
    "            logger.error(f\"Error: Could not open video source {input_source}\")\n",
    "            return\n",
    "\n",
    "        processing_times = []\n",
    "\n",
    "        try:\n",
    "           \n",
    "            while self.running and cap.isOpened():\n",
    "                start_time = time.time()\n",
    "\n",
    "                success, frame = cap.read()\n",
    "                if not success:\n",
    "                    logger.info(\"End of video\")\n",
    "                    break\n",
    "\n",
    "                # Process the frame\n",
    "                features, annotated_frame = self._process_frame(frame)\n",
    "\n",
    "                # If no pose detected\n",
    "                if features is None:\n",
    "                    await self._broadcast({\n",
    "                        \"type\": \"frame\",\n",
    "                        \"data\": frame_base64,\n",
    "                        \"prediction\": None,\n",
    "                        \"confidence\": None,\n",
    "                        \"rep_count\": self.rep_count\n",
    "                    })\n",
    "                    await asyncio.sleep(0.03)\n",
    "                    continue\n",
    "\n",
    "                # Make prediction if enough frames collected\n",
    "                if len(self.features_buffer) >= self.window_size:\n",
    "                    new_prediction, new_confidence = self._make_prediction()\n",
    "                    self.current_prediction, self.prediction_confidence = self._smooth_predictions(\n",
    "                        new_prediction, new_confidence)\n",
    "                    if self.current_prediction is not None:\n",
    "                        self._update_error_counts(self.current_prediction)\n",
    "\n",
    "                # Update rep count with current features\n",
    "                self._update_rep_count(features)\n",
    "\n",
    "                # Encode frame as base64\n",
    "                if annotated_frame is not None:\n",
    "                    _, buffer = cv2.imencode('.jpg', annotated_frame)\n",
    "                    frame_base64 = base64.b64encode(buffer).decode('utf-8')\n",
    "\n",
    "                    await self._broadcast({\n",
    "                        \"type\": \"frame\",\n",
    "                        \"data\": frame_base64,\n",
    "                        \"prediction\": self.current_prediction,\n",
    "                        \"confidence\": self.prediction_confidence,\n",
    "                        \"rep_count\": self.rep_count\n",
    "                    })\n",
    "\n",
    "                # Measure processing time\n",
    "                processing_time = time.time() - start_time\n",
    "                processing_times.append(processing_time)\n",
    "\n",
    "                # Maintain FPS at ~30\n",
    "                await asyncio.sleep(max(0, 0.03 - processing_time))\n",
    "\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error during video processing: {e}\")\n",
    "        finally:\n",
    "            cap.release()\n",
    "            logger.info(\"Video processing stopped\")\n",
    "\n",
    "\n",
    "                # Print report at the end\n",
    "        print(\"\\n\" + self.generate_report())\n",
    "        with open('report.txt', 'w') as f: f.write(self.generate_report())\n",
    "\n",
    "                \n",
    "\n",
    "    async def websocket_handler(self, websocket):\n",
    "        \"\"\"Handle WebSocket connections.\"\"\"\n",
    "        self.clients.add(websocket)\n",
    "        logger.info(f\"New client connected: {websocket.remote_address}\")\n",
    "        try:\n",
    "            async for message in websocket:\n",
    "                try:\n",
    "                    data = json.loads(message)\n",
    "                    action = data.get('action')\n",
    "                    logger.info(f\"Received action: {action}\")\n",
    "\n",
    "                    if action == 'connect':\n",
    "                        await websocket.send(json.dumps({\"status\": \"connected\"}))\n",
    "                    elif action == 'start_squats':\n",
    "                        if not self.running:\n",
    "                            self.running = True\n",
    "                            asyncio.create_task(self.start_processing())\n",
    "                            await websocket.send(json.dumps({\"status\": \"started\"}))\n",
    "                        else:\n",
    "                            await websocket.send(json.dumps({\"status\": \"already_running\"}))\n",
    "                    elif action == 'stop':\n",
    "                        if self.running:\n",
    "                            self.running = False\n",
    "                            await websocket.send(json.dumps({\"status\": \"stopped\"}))\n",
    "                        else:\n",
    "                            await websocket.send(json.dumps({\"status\": \"not_running\"}))\n",
    "                    else:\n",
    "                        logger.warning(f\"Unknown action: {action}\")\n",
    "                        await websocket.send(json.dumps({\"error\": \"Unknown action\"}))\n",
    "                except json.JSONDecodeError:\n",
    "                    logger.error(\"Invalid JSON received\")\n",
    "                    await websocket.send(json.dumps({\"error\": \"Invalid request format\"}))\n",
    "                except Exception as e:\n",
    "                    logger.error(f\"Error handling message: {str(e)}\")\n",
    "                    await websocket.send(json.dumps({\"error\": f\"Internal server error: {str(e)}\"}))\n",
    "        except websockets.exceptions.ConnectionClosed:\n",
    "            logger.info(f\"Client disconnected: {websocket.remote_address}\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Unexpected error in connection handler: {str(e)}\")\n",
    "        finally:\n",
    "            self.clients.remove(websocket)\n",
    "            logger.info(f\"Client removed: {websocket.remote_address}\")\n",
    "\n",
    "    \n",
    "    async def start_processing(self):\n",
    "        \"\"\"Start processing video frames and streaming via WebSocket.\"\"\"\n",
    "        self.running = True\n",
    "        await self.process_video(input_source=0)\n",
    "\n",
    "\n",
    "    async def _broadcast(self, message):\n",
    "        \"\"\"Broadcast a message to all connected clients.\"\"\"\n",
    "        if not self.clients:\n",
    "            return\n",
    "            \n",
    "        message_json = json.dumps(message)\n",
    "        await asyncio.gather(\n",
    "            *[client.send(message_json) for client in self.clients],\n",
    "            return_exceptions=True\n",
    "        )\n",
    "    \n",
    "    def broadcast_message(self, message):\n",
    "        \"\"\"Broadcast a message to all connected clients from any thread.\"\"\"\n",
    "        if self.event_loop and self.clients:\n",
    "            asyncio.run_coroutine_threadsafe(self._broadcast(message), self.event_loop)\n",
    "            \n",
    "    def stop_processing(self):\n",
    "        \"\"\"Stop processing video frames.\"\"\"\n",
    "        self.running = False\n",
    "        logger.info(\"Stopping processing\")\n",
    "        # Wait for the detector thread to finish\n",
    "        if self.detector_thread and self.detector_thread.is_alive():\n",
    "            self.detector_thread.join(timeout=5.0)\n",
    "        # Camera will be released in the process_frames method's finally block\n",
    "\n",
    "\n",
    "\n",
    "    def start_server(self, host='localhost', port=8765):\n",
    "        \"\"\"Start the WebSocket server.\"\"\"\n",
    "        def run_event_loop(loop):\n",
    "            asyncio.set_event_loop(loop)\n",
    "            loop.run_forever()\n",
    "\n",
    "        # Create a new event loop for the WebSocket server\n",
    "        self.event_loop = asyncio.new_event_loop()\n",
    "\n",
    "        # Start the event loop in a separate thread\n",
    "        server_thread = threading.Thread(\n",
    "            target=run_event_loop,\n",
    "            args=(self.event_loop,),\n",
    "            daemon=True\n",
    "        )\n",
    "        server_thread.start()\n",
    "\n",
    "        # Schedule the WebSocket server to start\n",
    "        asyncio.run_coroutine_threadsafe(self._start_websocket_server(host, port), self.event_loop)\n",
    "\n",
    "        logger.info(f\"WebSocket server started on ws://{host}:{port}/ws\")\n",
    "\n",
    "\n",
    "async def _start_websocket_server(self, host, port):\n",
    "        \"\"\"Start the WebSocket server in the event loop.\"\"\"\n",
    "        self.server = await websockets.serve(\n",
    "            partial(self.websocket_handler),  # Bind self to the method\n",
    "            host,\n",
    "            port,\n",
    "            ping_interval=30\n",
    "        )\n",
    "\n",
    "    def stop_server(self):\n",
    "        \"\"\"Stop the WebSocket server.\"\"\"\n",
    "        if self.server:\n",
    "            self.server.close()\n",
    "            asyncio.run_coroutine_threadsafe(self.server.wait_closed(), self.event_loop)\n",
    "            self.event_loop.call_soon_threadsafe(self.event_loop.stop)\n",
    "            logger.info(\"WebSocket server stopped\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b272f954-c810-4699-83ca-f457f707e284",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "432af70b-2cfa-4139-8c8c-ed0b180d62ac",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be07cd63-9e0d-465d-bd5a-7b8c5329810e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42c6bf83-30c0-40e4-be2d-33a80c9d86ba",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95091a1c-da12-4366-9a1b-e3d507a16b39",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31b4cdee-a9df-473b-9eb2-15fae5539b0b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b4d004b-cc57-4d7a-9854-21119ad7b700",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def process_video(self, frame):\n",
    "        \"\"\"Process a single frame and return data to broadcast.\"\"\"\n",
    "        # Process the frame with MediaPipe Pose\n",
    "        results = self.pose.process(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))\n",
    "        annotated_frame = frame.copy()  # Create a copy to annotate\n",
    "\n",
    "        if results.pose_landmarks:\n",
    "            self.mp_drawing.draw_landmarks(annotated_frame, results.pose_landmarks, self.mp_pose.POSE_CONNECTIONS)\n",
    "            errors = self.check_warrior_pose(results.pose_landmarks.landmark)\n",
    "\n",
    "            # Update frame count and recording logic\n",
    "            self.frame_count += 1\n",
    "            if self.frame_count > self.delay_frames and not self.recording:\n",
    "                self.recording = True\n",
    "                self.start_frame = self.frame_count\n",
    "            if self.recording and (self.frame_count - self.start_frame) <= self.record_frames:\n",
    "                if not errors:\n",
    "                    self.report[\"good_form_frames\"] += 1\n",
    "                for error in errors:\n",
    "                    if error not in self.report[\"error_counts\"]:\n",
    "                        self.report[\"error_counts\"][error] = 0\n",
    "                    self.report[\"error_counts\"][error] += 1\n",
    "\n",
    "            # Display errors or \"Correct Form\" on the frame\n",
    "            if errors:\n",
    "                for i, error in enumerate(errors):\n",
    "                    cv2.putText(annotated_frame, error, (10, 30 + i * 30), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 0, 255), 2)\n",
    "            else:\n",
    "                cv2.putText(annotated_frame, \"Correct Form\", (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 0), 2)\n",
    "\n",
    "        # Encode frame as base64 and return data\n",
    "        frame_base64 = self._encode_frame(annotated_frame)\n",
    "        return {\n",
    "            \"type\": \"frame\",\n",
    "            \"data\": frame_base64,\n",
    "            \"good_form_frames\": self.report[\"good_form_frames\"],\n",
    "            \"error_counts\": self.report[\"error_counts\"],\n",
    "            \"recording\": self.recording,\n",
    "            \"frame_count\": self.frame_count - self.start_frame if self.recording else 0\n",
    "        }\n",
    "\n",
    "    def _encode_frame(self, frame):\n",
    "        \"\"\"Encode frame as base64.\"\"\"\n",
    "        _, buffer = cv2.imencode('.jpg', frame)\n",
    "        return base64.b64encode(buffer).decode('utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc3d6147-f4f3-4c43-88ea-5cd4c9ac3026",
   "metadata": {},
   "outputs": [],
   "source": [
    "class WarriorPoseAnalyzer:\n",
    "    def __init__(self, record_seconds=10, fps=30):\n",
    "        # Initialize MediaPipe Pose\n",
    "        self.mp_pose = mp.solutions.pose\n",
    "        self.mp_drawing = mp.solutions.drawing_utils\n",
    "        self.pose = self.mp_pose.Pose()\n",
    "\n",
    "        # Define adjustable thresholds\n",
    "        self.THRESHOLDS = {\n",
    "            \"front_knee_angle\": (70, 120),\n",
    "            \"back_leg_angle\": (150, 180),\n",
    "            \"hip_orientation\": (0, 30),\n",
    "            \"torso_angle\": (0, 100),\n",
    "            \"arm_angle\": (160, 180),\n",
    "            \"shoulder_hip_alignment\": (0, 40)\n",
    "        }\n",
    "\n",
    "        # Recording settings\n",
    "        self.fps = fps\n",
    "        self.delay_frames = 3 * fps  # 3 seconds delay\n",
    "        self.record_frames = record_seconds * fps  # Number of frames to record\n",
    "        self.frame_count = 0\n",
    "        self.recording = False\n",
    "        self.report = {\n",
    "            \"good_form_frames\": 0,\n",
    "            \"error_counts\": defaultdict(int)  # Tracks each error's frequency\n",
    "        }\n",
    "\n",
    "    def calculate_angle(self, p1, p2, p3):\n",
    "        \"\"\"Calculate the angle between three points in degrees.\"\"\"\n",
    "        a = np.array(p1)\n",
    "        b = np.array(p2)\n",
    "        c = np.array(p3)\n",
    "        ba = a - b\n",
    "        bc = c - b\n",
    "        cosine_angle = np.dot(ba, bc) / (np.linalg.norm(ba) * np.linalg.norm(bc))\n",
    "        angle = np.arccos(cosine_angle) * 180 / np.pi\n",
    "        return angle\n",
    "\n",
    "    def check_warrior_pose(self, landmarks):\n",
    "        \"\"\"Analyze Warrior II pose and return top 3 errors.\"\"\"\n",
    "        errors = []\n",
    "\n",
    "        l_hip = [landmarks[self.mp_pose.PoseLandmark.LEFT_HIP].x, landmarks[self.mp_pose.PoseLandmark.LEFT_HIP].y]\n",
    "        r_hip = [landmarks[self.mp_pose.PoseLandmark.RIGHT_HIP].x, landmarks[self.mp_pose.PoseLandmark.RIGHT_HIP].y]\n",
    "        l_knee = [landmarks[self.mp_pose.PoseLandmark.LEFT_KNEE].x, landmarks[self.mp_pose.PoseLandmark.LEFT_KNEE].y]\n",
    "        r_knee = [landmarks[self.mp_pose.PoseLandmark.RIGHT_KNEE].x, landmarks[self.mp_pose.PoseLandmark.RIGHT_KNEE].y]\n",
    "        l_ankle = [landmarks[self.mp_pose.PoseLandmark.LEFT_ANKLE].x, landmarks[self.mp_pose.PoseLandmark.LEFT_ANKLE].y]\n",
    "        r_ankle = [landmarks[self.mp_pose.PoseLandmark.RIGHT_ANKLE].x, landmarks[self.mp_pose.PoseLandmark.RIGHT_ANKLE].y]\n",
    "        l_shoulder = [landmarks[self.mp_pose.PoseLandmark.LEFT_SHOULDER].x, landmarks[self.mp_pose.PoseLandmark.LEFT_SHOULDER].y]\n",
    "        r_shoulder = [landmarks[self.mp_pose.PoseLandmark.RIGHT_SHOULDER].x, landmarks[self.mp_pose.PoseLandmark.RIGHT_SHOULDER].y]\n",
    "        l_elbow = [landmarks[self.mp_pose.PoseLandmark.LEFT_ELBOW].x, landmarks[self.mp_pose.PoseLandmark.LEFT_ELBOW].y]\n",
    "        r_elbow = [landmarks[self.mp_pose.PoseLandmark.RIGHT_ELBOW].x, landmarks[self.mp_pose.PoseLandmark.RIGHT_ELBOW].y]\n",
    "        l_wrist = [landmarks[self.mp_pose.PoseLandmark.LEFT_WRIST].x, landmarks[self.mp_pose.PoseLandmark.LEFT_WRIST].y]\n",
    "        r_wrist = [landmarks[self.mp_pose.PoseLandmark.RIGHT_WRIST].x, landmarks[self.mp_pose.PoseLandmark.RIGHT_WRIST].y]\n",
    "\n",
    "        left_knee_angle = self.calculate_angle(l_hip, l_knee, l_ankle)\n",
    "        right_knee_angle = self.calculate_angle(r_hip, r_knee, r_ankle)\n",
    "        \n",
    "        if left_knee_angle < right_knee_angle:\n",
    "            front_hip, front_knee, front_ankle = l_hip, l_knee, l_ankle\n",
    "            back_hip, back_knee, back_ankle = r_hip, r_knee, r_ankle\n",
    "            front_knee_angle = left_knee_angle\n",
    "            back_leg_angle = right_knee_angle\n",
    "            hip_angle = self.calculate_angle(l_hip, r_hip, [r_hip[0] + 1, r_hip[1]])\n",
    "        else:\n",
    "            front_hip, front_knee, front_ankle = r_hip, r_knee, r_ankle\n",
    "            back_hip, back_knee, back_ankle = l_hip, l_knee, l_ankle\n",
    "            front_knee_angle = right_knee_angle\n",
    "            back_leg_angle = left_knee_angle\n",
    "            hip_angle = self.calculate_angle(r_hip, l_hip, [l_hip[0] - 1, l_hip[1]])\n",
    "\n",
    "        torso_angle = self.calculate_angle(\n",
    "            [(l_hip[0] + r_hip[0]) / 2, (l_hip[1] + r_hip[1]) / 2],\n",
    "            [(l_shoulder[0] + r_shoulder[0]) / 2, (l_shoulder[1] + r_shoulder[1]) / 2],\n",
    "            [0, 1]\n",
    "        )\n",
    "        l_arm_angle = self.calculate_angle(l_shoulder, l_elbow, l_wrist)\n",
    "        r_arm_angle = self.calculate_angle(r_shoulder, r_elbow, r_wrist)\n",
    "        shoulder_hip_angle = self.calculate_angle(l_shoulder, r_shoulder, r_hip)\n",
    "\n",
    "        if not self.THRESHOLDS[\"front_knee_angle\"][0] <= front_knee_angle <= self.THRESHOLDS[\"front_knee_angle\"][1]:\n",
    "            errors.append(\"Bend your front knee more\" if front_knee_angle > 100 else \"Straighten your front knee slightly\")\n",
    "        if not self.THRESHOLDS[\"back_leg_angle\"][0] <= back_leg_angle <= self.THRESHOLDS[\"back_leg_angle\"][1]:\n",
    "            errors.append(\"Straighten your back leg\" if back_leg_angle < 160 else \"Relax your back leg slightly\")\n",
    "        \n",
    "        if not self.THRESHOLDS[\"hip_orientation\"][0] <= hip_angle <= self.THRESHOLDS[\"hip_orientation\"][1]:\n",
    "            if front_hip == r_hip:\n",
    "                if r_hip[1] > l_hip[1]:\n",
    "                    errors.append(\"Level your hips; right hip is too high\")\n",
    "                else:\n",
    "                    errors.append(\"Level your hips; left hip is too high\")\n",
    "            else:\n",
    "                if l_hip[1] > r_hip[1]:\n",
    "                    errors.append(\"Level your hips; left hip is too high\")\n",
    "                else:\n",
    "                    errors.append(\"Level your hips; right hip is too high\")\n",
    "\n",
    "        if not self.THRESHOLDS[\"arm_angle\"][0] <= l_arm_angle <= self.THRESHOLDS[\"arm_angle\"][1] or \\\n",
    "           not self.THRESHOLDS[\"arm_angle\"][0] <= r_arm_angle <= self.THRESHOLDS[\"arm_angle\"][1]:\n",
    "            errors.append(\"Raise your arms to shoulder level\" if l_arm_angle < 170 or r_arm_angle < 170 else \"Extend your arms fully\")\n",
    "\n",
    "        return errors[:3]\n",
    "\n",
    "    def generate_report(self):\n",
    "        \"\"\"Generate and print an exercise report.\"\"\"\n",
    "        total_recorded_frames = self.record_frames\n",
    "        good_form_seconds = self.report[\"good_form_frames\"] / self.fps\n",
    "        total_seconds = total_recorded_frames / self.fps\n",
    "\n",
    "        print(\"\\n--- Warrior II Exercise Report ---\")\n",
    "        print(f\"Total Recorded Time: {total_seconds:.2f} seconds\")\n",
    "        print(f\"Good Form Duration: {good_form_seconds:.2f} seconds ({(good_form_seconds / total_seconds) * 100:.1f}%)\")\n",
    "        print(\"Errors Detected:\")\n",
    "        if self.report[\"error_counts\"]:\n",
    "            for error, count in self.report[\"error_counts\"].items():\n",
    "                error_seconds = count / self.fps\n",
    "                print(f\"  - '{error}': {count} frames ({error_seconds:.2f} seconds, {(count / total_recorded_frames) * 100:.1f}%)\")\n",
    "        else:\n",
    "            print(\"  - No errors detected!\")\n",
    "        print(\"--------------------------------\\n\")\n",
    "\n",
    "\n",
    "    async def process_video(self, frame):\n",
    "        \"\"\"Process a single frame and return data to broadcast.\"\"\"\n",
    "        # Process the frame with MediaPipe Pose\n",
    "        results = self.pose.process(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))\n",
    "        annotated_frame = frame.copy()  # Create a copy to annotate\n",
    "\n",
    "        if results.pose_landmarks:\n",
    "            self.mp_drawing.draw_landmarks(annotated_frame, results.pose_landmarks, self.mp_pose.POSE_CONNECTIONS)\n",
    "            errors = self.check_warrior_pose(results.pose_landmarks.landmark)\n",
    "\n",
    "            # Update frame count and recording logic\n",
    "            self.frame_count += 1\n",
    "            if self.frame_count > self.delay_frames and not self.recording:\n",
    "                self.recording = True\n",
    "                self.start_frame = self.frame_count\n",
    "            if self.recording and (self.frame_count - self.start_frame) <= self.record_frames:\n",
    "                if not errors:\n",
    "                    self.report[\"good_form_frames\"] += 1\n",
    "                for error in errors:\n",
    "                    if error not in self.report[\"error_counts\"]:\n",
    "                        self.report[\"error_counts\"][error] = 0\n",
    "                    self.report[\"error_counts\"][error] += 1\n",
    "\n",
    "            # Display errors or \"Correct Form\" on the frame\n",
    "            if errors:\n",
    "                for i, error in enumerate(errors):\n",
    "                    cv2.putText(annotated_frame, error, (10, 30 + i * 30), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 0, 255), 2)\n",
    "            else:\n",
    "                cv2.putText(annotated_frame, \"Correct Form\", (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 0), 2)\n",
    "\n",
    "        # Encode frame as base64 and return data\n",
    "        frame_base64 = self._encode_frame(annotated_frame)\n",
    "        return {\n",
    "            \"type\": \"frame\",\n",
    "            \"data\": frame_base64,\n",
    "            \"good_form_frames\": self.report[\"good_form_frames\"],\n",
    "            \"error_counts\": self.report[\"error_counts\"],\n",
    "            \"recording\": self.recording,\n",
    "            \"frame_count\": self.frame_count - self.start_frame if self.recording else 0\n",
    "        }\n",
    "\n",
    "    def _encode_frame(self, frame):\n",
    "        \"\"\"Encode frame as base64.\"\"\"\n",
    "        _, buffer = cv2.imencode('.jpg', frame)\n",
    "        return base64.b64encode(buffer).decode('utf-8')\n",
    "\n",
    "    def __del__(self):\n",
    "        self.pose.close()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fypvis",
   "language": "python",
   "name": "fypvis"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
