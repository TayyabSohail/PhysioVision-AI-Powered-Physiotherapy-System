import cv2
import mediapipe as mp
import numpy as np
import pickle as pickle
import logging

logger = logging.getLogger(__name__)


class LungesAnalyzer:
    def __init__(self):
        # Initialize MediaPipe Pose
        self.mp_pose = mp.solutions.pose
        self.pose = self.mp_pose.Pose(
            static_image_mode=False,
            model_complexity=2,
            min_detection_confidence=0.5,
            min_tracking_confidence=0.5
        )
        self.mp_drawing = mp.solutions.drawing_utils

        self.threshold_test = {"lower": None,
                 "higher": None}
        
        # Initialize model components
        self.scaler = None
        self.pca = None
        self.model = None
        self.is_trained = False
        
        # Define the landmarks we're interested in (hip, knee, ankle only)
        self.target_landmarks = [
            'LEFT_HIP', 'LEFT_KNEE', 'LEFT_ANKLE',
            'RIGHT_HIP', 'RIGHT_KNEE', 'RIGHT_ANKLE'
        ]
        
    def extract_keypoints(self, frame):
        """Extract hip, knee, and ankle keypoints from a single frame."""
        # Convert BGR to RGB
        frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
        
        # Process the frame
        results = self.pose.process(frame_rgb)
        
        if not results.pose_landmarks:
            return None, None
        
        # Extract only hip, knee, ankle keypoints
        keypoints = []
        landmark_dict = {}
        
        for i, landmark in enumerate(results.pose_landmarks.landmark):
            name = self.mp_pose.PoseLandmark(i).name
            if name in self.target_landmarks:
                landmark_dict[name] = [landmark.x, landmark.y, landmark.z]
        
        # Determine leading leg
        knee_r = landmark_dict['RIGHT_KNEE'][:2]  # Just x,y for position comparison
        knee_l = landmark_dict['LEFT_KNEE'][:2]
        
        # Lower y-value means higher in the image (closer to top of frame)
        if knee_r[1] < knee_l[1]:  # Right knee is higher in the frame
            leading_leg = "Right"  # Right leg is forward
        else:
            leading_leg = "Left"  # Left leg is forward
            
        # Extract features in a consistent order
        for name in self.target_landmarks:
            keypoints.extend(landmark_dict[name])
            
        return np.array(keypoints), leading_leg
    
    def normalize_side(self, keypoints, leading_leg):
        """
        Normalize left/right sides to treat them as the same movement pattern.
        This maps all lunges to a standardized form regardless of which leg is forward.
        """
        # Reshape keypoints to have landmarks as rows with [x,y,z] columns
        # We have 6 landmarks (L/R hip, knee, ankle) with 3 coordinates each
        landmarks = keypoints.reshape(6, 3)
        
        # Standardize to always have the same leg configuration
        # If right leg is forward but we want left leg to be our standard (or vice versa)
        if leading_leg == "Right":
            # Swap left and right sides
            temp = np.copy(landmarks[0:3])
            landmarks[0:3] = landmarks[3:6]
            landmarks[3:6] = temp
        
        # Return flattened normalized keypoints
        return landmarks.flatten()
    
    def calculate_lunge_features(self, keypoints, original_leading_leg):
        """Calculate important angles and distances for lunge form assessment."""
        # Reshape keypoints to have landmarks as rows with [x,y,z] columns
        landmarks = keypoints.reshape(6, 3)
        
        # Extract individual landmarks
        front_hip = landmarks[0]    # Using left as front leg (after normalization)
        front_knee = landmarks[1]
        front_ankle = landmarks[2]
        
        # Calculate angle between three points
        def calculate_angle(a, b, c):
            a = np.array(a)
            b = np.array(b)
            c = np.array(c)
            
            ba = a - b
            bc = c - b
            
            cosine_angle = np.dot(ba, bc) / (np.linalg.norm(ba) * np.linalg.norm(bc))
            # Clip to avoid numerical errors
            cosine_angle = np.clip(cosine_angle, -1.0, 1.0)
            angle = np.arccos(cosine_angle)
            return np.degrees(angle)
        
        # Calculate relevant angles and distances
        features = {}
        
        # Front leg angles
        features['front_knee_angle'] = calculate_angle(front_hip, front_knee, front_ankle)
        
        # Store the original leading leg for reference
        features['leading_leg'] = original_leading_leg
        
        return features
    
    def train_model(self, video_paths):
        """Train the model on videos with correct form."""
        all_keypoints = []
        all_features = []
        
        for video_path in video_paths:
            print(f"Processing {video_path}...")
            keypoints, features = self.process_video(video_path)
            if len(keypoints) > 0:  # Check if any valid frames were found
                all_keypoints.extend(keypoints)
                all_features.extend(features)
            else:
                print(f"Warning: No valid pose data found in {video_path}")
        
        if len(all_keypoints) == 0:
            raise ValueError("No valid pose data found in any training videos.")
            
        # Convert to numpy arrays
        all_keypoints = np.array(all_keypoints)
        
        print(f"Total frames collected: {len(all_keypoints)}")
        
        # Normalize the data
        self.scaler = StandardScaler()
        keypoints_scaled = self.scaler.fit_transform(all_keypoints)
        
        # Apply PCA for dimensionality reduction
        self.pca = PCA(n_components=0.95)  # Retain 95% of variance
        keypoints_pca = self.pca.fit_transform(keypoints_scaled)
        
        # Train the Isolation Forest model
        self.model = IsolationForest(contamination=0.05, random_state=42)
        self.model.fit(keypoints_pca)
        
        # Save feature statistics for rule-based feedback
        feature_df = pd.DataFrame(all_features)
        # Filter out non-numeric columns for statistics
        numeric_features = feature_df.select_dtypes(include=[np.number])
        self.feature_means = numeric_features.mean()
        self.feature_stds = numeric_features.std()
        
        self.is_trained = True
        print("Model training complete!")
        
        # Save the model
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        model_filename = f"lunge_model_{timestamp}.pkl"
        with open(model_filename, 'wb') as f:
            pickle.dump({
                'scaler': self.scaler,
                'pca': self.pca,
                'model': self.model,
                'feature_means': self.feature_means,
                'feature_stds': self.feature_stds
            }, f)
        print(f"Model saved as {model_filename}")
    
    def load_model(self, model_path):
        try:
            print("Attempting to load model...")
            with open(model_path, 'rb') as f:
                model_data = pickle.load(f)
                self.scaler = model_data['scaler']
                self.pca = model_data['pca']
                self.model = model_data['model']
                self.feature_means = model_data['feature_means']
                self.feature_stds = model_data['feature_stds']
            self.is_trained = True
            print("Model loaded successfully!")
        except Exception as e:
            print(f"Error loading model: {e}")



    def reset_counters(self):
        """Reset counters and data storage."""
        self.frame_count = 0
        self.frames_keypoints = []
        self.features_data = []

    async def process_video(self, frame):
        """Process a single frame and return data to broadcast."""
        self.frame_count += 1

        # Extract keypoints and leading leg from the frame
        keypoints, leading_leg = self.extract_keypoints(frame)

        if keypoints is not None:
            # Normalize based on which leg is leading
            normalized_keypoints = self.normalize_side(keypoints, leading_leg)
            self.frames_keypoints.append(normalized_keypoints)

            # Calculate features for specific feedback
            features = self.calculate_lunge_features(normalized_keypoints, leading_leg)
            self.features_data.append(features)

            # Annotate frame (optional, for visualization)
            annotated_frame = frame.copy()
            cv2.putText(annotated_frame, f"Frame: {self.frame_count}", (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255, 255, 255), 2)
            if leading_leg:
                cv2.putText(annotated_frame, f"Leading Leg: {leading_leg}", (10, 60), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 0), 2)
        else:
            annotated_frame = frame.copy()
            cv2.putText(annotated_frame, "No keypoints detected", (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 0, 255), 2)

        # Encode frame as base64 and return data
        frame_base64 = self._encode_frame(annotated_frame)
        return {
            "type": "frame",
            "data": frame_base64,
            "leading_leg": leading_leg if keypoints is not None else None,
            "frame_count": self.frame_count,
            "keypoints_detected": keypoints is not None
        }

    def _encode_frame(self, frame):
        """Encode frame as base64."""
        _, buffer = cv2.imencode('.jpg', frame)
        return base64.b64encode(buffer).decode('utf-8')

    def generate_report(self):
        """Generate and return an exercise report."""
        report = "\n--- Lunges Exercise Report ---\n"
        report += f"Total Frames Processed: {self.frame_count}\n"
        report += f"Frames with Keypoints Detected: {len(self.frames_keypoints)}\n"
        if self.features_data:
            report += "Feature Summary (example):\n"
            # Add summary stats if desired, e.g., average depth or knee angle
            for i, features in enumerate(self.features_data[:5]):  # Limit to first 5 for brevity
                report += f"  - Frame {i+1}: {features}\n"
            if len(self.features_data) > 5:
                report += f"  - ...and {len(self.features_data) - 5} more frames\n"
        else:
            report += "No features calculated (no keypoints detected).\n"
        report += "--------------------------------\n"
        return report
    
    def detect_form(self, frame):
        """Detect lunge form in a single frame."""
        if not self.is_trained:
            raise ValueError("Model not trained. Please train or load a model first.")
        
        keypoints, leading_leg = self.extract_keypoints(frame)
        if keypoints is None:
            return False, "No person detected", None, None
        
        # Normalize based on which leg is leading
        normalized_keypoints = self.normalize_side(keypoints, leading_leg)
        
        # Calculate features for specific feedback
        features = self.calculate_lunge_features(normalized_keypoints, leading_leg)
        
        # Prepare keypoints for prediction
        keypoints_scaled = self.scaler.transform(normalized_keypoints.reshape(1, -1))
        keypoints_pca = self.pca.transform(keypoints_scaled)
        
        # Make prediction
        prediction = self.model.predict(keypoints_pca)[0]
        score = self.model.score_samples(keypoints_pca)[0]
        
        # Check if form is correct
        is_correct = prediction == 1
        
        # Prepare feedback
        feedback = ""
        errors = {}
        
        if not is_correct:
    for feature_name, feature_value in features.items():
        if feature_name == 'leading_leg':
            continue
            
        if "knee_angle" in feature_name:
            if "front" in feature_name:
                if feature_value < 70:
                    message = f"{leading_leg} leg: Bend your knee more."
                    errors[feature_name] = {"message": message, "value": feature_value, "ideal": 90}
                    feedback += message + " "
                elif feature_value > 110:
                    message = f"{leading_leg} leg: Don't bend your knee too much."
                    errors[feature_name] = {"message": message, "value": feature_value, "ideal": 90}
                    feedback += message + " "
            
            if not feedback:
                feedback = "Form needs improvement. Check your overall posture."
        else:
            feedback = "Good form!"
        
        return is_correct, feedback, features, errors
    
    def analyze_video(self, video_path= 0, output_path=None):
        """Analyze a video file and output a video with form analysis."""
        if not self.is_trained:
            raise ValueError("Model not trained. Please train or load a model first.")
        
        cap = cv2.VideoCapture(video_path)
        
        # Get video properties
        width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))
        height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))
        fps = cap.get(cv2.CAP_PROP_FPS)
        frame_count = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
        
        # Create output video writer if path is provided
        if output_path:
            fourcc = cv2.VideoWriter_fourcc(*'mp4v')
            out = cv2.VideoWriter(output_path, fourcc, fps, (width, height))
        
        frame_idx = 0
        while cap.isOpened():
            ret, frame = cap.read()
            if not ret:
                break
                
            # Process the frame
            is_correct, feedback, features, errors = self.detect_form(frame)
            
            # Draw the pose on the frame
            frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
            results = self.pose.process(frame_rgb)
            frame = cv2.cvtColor(frame_rgb, cv2.COLOR_RGB2BGR)
            
            if results.pose_landmarks:
                # Extract landmarks and leading leg information
                leading_leg = features.get('leading_leg', 'Unknown') if features else 'Unknown'
                
                # Custom drawing to highlight only hips, knees, ankles
                landmarks = results.pose_landmarks
                
                # Create a custom connection list for hip-knee-ankle
                custom_connections = [
                    (self.mp_pose.PoseLandmark.LEFT_HIP, self.mp_pose.PoseLandmark.LEFT_KNEE),
                    (self.mp_pose.PoseLandmark.LEFT_KNEE, self.mp_pose.PoseLandmark.LEFT_ANKLE),
                    (self.mp_pose.PoseLandmark.RIGHT_HIP, self.mp_pose.PoseLandmark.RIGHT_KNEE),
                    (self.mp_pose.PoseLandmark.RIGHT_KNEE, self.mp_pose.PoseLandmark.RIGHT_ANKLE),
                ]
                
                # Highlight the leading leg with a different color
                left_color = (0, 255, 0) if leading_leg == "Left" else (255, 0, 0)  # Green for leading, blue for back
                right_color = (0, 255, 0) if leading_leg == "Right" else (255, 0, 0)
                
                # Draw only the landmarks we care about
                landmark_ids = [
                    (self.mp_pose.PoseLandmark.LEFT_HIP, left_color),
                    (self.mp_pose.PoseLandmark.LEFT_KNEE, left_color),
                    (self.mp_pose.PoseLandmark.LEFT_ANKLE, left_color),
                    (self.mp_pose.PoseLandmark.RIGHT_HIP, right_color),
                    (self.mp_pose.PoseLandmark.RIGHT_KNEE, right_color),
                    (self.mp_pose.PoseLandmark.RIGHT_ANKLE, right_color)
                ]
                
                for landmark_id, color in landmark_ids:
                    landmark = landmarks.landmark[landmark_id]
                    h, w, c = frame.shape
                    cx, cy = int(landmark.x * w), int(landmark.y * h)
                    cv2.circle(frame, (cx, cy), 10, color, -1)
                
                # Draw connections with correct colors
                for connection in [
                    (self.mp_pose.PoseLandmark.LEFT_HIP, self.mp_pose.PoseLandmark.LEFT_KNEE, left_color),
                    (self.mp_pose.PoseLandmark.LEFT_KNEE, self.mp_pose.PoseLandmark.LEFT_ANKLE, left_color),
                    (self.mp_pose.PoseLandmark.RIGHT_HIP, self.mp_pose.PoseLandmark.RIGHT_KNEE, right_color),
                    (self.mp_pose.PoseLandmark.RIGHT_KNEE, self.mp_pose.PoseLandmark.RIGHT_ANKLE, right_color),
                ]:
                    start_idx = connection[0].value
                    end_idx = connection[1].value
                    connection_color = connection[2]
                    
                    start = landmarks.landmark[start_idx]
                    end = landmarks.landmark[end_idx]
                    
                    h, w, c = frame.shape
                    start_point = (int(start.x * w), int(start.y * h))
                    end_point = (int(end.x * w), int(end.y * h))
                    
                    cv2.line(frame, start_point, end_point, connection_color, 3)
            
            # Display leading leg information
            if features and 'leading_leg' in features:
                cv2.putText(frame, f"Leading Leg: {features['leading_leg']}", 
                            (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255, 255, 255), 2)
            
            # Display feedback
            color = (0, 255, 0) if is_correct else (0, 0, 255)
            cv2.putText(frame, feedback, (10, 60), cv2.FONT_HERSHEY_SIMPLEX, 0.7, color, 2)
            
            # Display errors with visual indicators
            if errors:
                y_pos = 90
                for feature_name, error_info in errors.items():
                    message = error_info["message"]
                    cv2.putText(frame, message, (10, y_pos), 
                                cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 0, 255), 2)
                    y_pos += 30
            
            # Display features if available
            if features:
                y_pos = height - 150  # Start from bottom of frame
                for feature_name, feature_value in features.items():
                    if feature_name == 'leading_leg':
                        continue
                    # Format feature value nicely
                    if isinstance(feature_value, (int, float)):
                        value_str = f"{feature_value:.1f}"
                    else:
                        value_str = str(feature_value)
                    
                    cv2.putText(frame, f"{feature_name}: {value_str}", (10, y_pos), 
                                cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 1)
                    y_pos += 20
            
            # Display progress
            frame_idx += 1
            progress = frame_idx / frame_count * 100
            cv2.putText(frame, f"Progress: {progress:.1f}%", (width - 200, 30), 
                        cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255, 255, 255), 2)
            
            # Write frame to output video if specified
            if output_path:
                out.write(frame)
            
            # Show the frame
            cv2.imshow('Lunge Form Analysis', frame)
            
            # Break loop if 'q' is pressed
            if cv2.waitKey(1) & 0xFF == ord('q'):
                break
        
        cap.release()
        if output_path:
            out.release()
        cv2.destroyAllWindows()
        print(f"Video analysis complete! Output saved to {output_path if output_path else 'not saved'}")